{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfc6916",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspam.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mANSI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39m to_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspam.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Jupyterpython\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "df = pd. read_csv('spam.csv', encoding='ANSI')\n",
    "df. to_csv('spam.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb25226d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44def8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a95a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Data Cleaning \n",
    "#2. EDA \n",
    "#3. Text Preprocessing \n",
    "#4. Model Building \n",
    "#5. evaluation\n",
    "#6. improvements \n",
    "#7. creating website \n",
    "#8 . deploy to heroku "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83b6dd",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a1fe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>ham</td>\n",
       "      <td>Its sarcasm.. .nt scarcasim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>ham</td>\n",
       "      <td>What about this one then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2237</th>\n",
       "      <td>ham</td>\n",
       "      <td>Give her something to drink, if she takes it a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>ham</td>\n",
       "      <td>Then u ask darren go n pick u lor... But i oso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dear how is chechi. Did you talk to her</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1                                                 v2\n",
       "2654  ham                        Its sarcasm.. .nt scarcasim\n",
       "5529  ham                          What about this one then.\n",
       "2237  ham  Give her something to drink, if she takes it a...\n",
       "2822  ham  Then u ask darren go n pick u lor... But i oso...\n",
       "401   ham            Dear how is chechi. Did you talk to her"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59eac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns \n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3f0ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cb378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since in target columns its written either ham or spam so we apply label encoding here \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "df['target']=encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec740359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       0  Go until jurong point, crazy.. Available only ...\n",
       "1       0                      Ok lar... Joking wif u oni...\n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       0  U dun say so early hor... U c already then say...\n",
       "4       0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spam = 1 , ham =0 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0830452d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking missing values \n",
    "df.isnull().sum()  #no missing values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "973061e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate values \n",
    "df.duplicated().sum() #403 duplicate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccba18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates \n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa90dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()  #no duplicates present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef43786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986542b",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01ac6b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4516\n",
       "1     653\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()  #ham is 4516 and spam is 653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5825f818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x1c05abf69d0>,\n",
       "  <matplotlib.patches.Wedge at 0x1c05ac10130>],\n",
       " [Text(-1.0144997251399075, 0.4251944351600247, 'ham'),\n",
       "  Text(1.014499764949479, -0.4251943401757036, 'spam')],\n",
       " [Text(-0.5533634864399495, 0.23192423736001344, '87.37'),\n",
       "  Text(0.5533635081542612, -0.23192418555038377, '12.63')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPAAAADnCAYAAAAghtuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAYBElEQVR4nO3deZgU1b3G8e+ZfRg2WURZYimIKKKCCigoGJegFYNRE1ySG1Fzo0bjrpV4o6PRWJrE3SQ37hqzaMxVtBQwigtqcEEQFcGtjLIj2LLN9HT3uX9UAcM6PTPdfaq6f5/n6Ydhpqvr9XFezunqqlNKa40QIp7KTAcQQrSdFFiIGJMCCxFjUmAhYkwKLESMSYGFiDEpsBAxJgUWIsakwELEmBRYiBiTAgsRY1JgIWJMCixEjEmBhYgxKbAQMSYFFiLGpMBCxJgUWIgYkwILEWNSYCFiTAosRIxJgYWIMSmwEDEmBRYixqTAQsSYFFiIGJMCCxFjFaYDiJZZjtcJ2A3YFbCAnkCXbTyqgSTQ2Oyx/u/rgMXA55s/fNdeXbD/IJEzSm5uFh2W45UD+wEjgMHAnsBeQK8C7D4BzANmA7OAt4FZvmuvK8C+RRtJgQ2yHK8OGAmMDh8jgY5GQ20qBbwDzACmA1N81/7SbCTRnBS4wCzH2wc4ATgaGEq83sZkCMrsAU/7rv224TwlTwpcAJbjDQNOJCjuQMNxcmkh8DQwCZjsu3aT4TwlRwqcJ5bjDQZOIyjtrmbTFMQS4EHgHt+155kOUyqkwDkUHoT6DnAecJjhOCa9AtwNPOq79hrTYYqZFDgHLMfrDpwJnA3sYjhOlKwC/gzc6Lu2bzhLUZICt4PleAOBy4FTgBrDcaIsBTwE/Np37Y9MhykmUuA2sBxvN+BK4AdAueE4cZIG/gpc57v2B6bDFAMpcCtYjtcLuIpgulxpOE6cZYB/AFfKAa/2kQJnITzh4mLgUqJ1okXcNQF3AFf7rp0wHSaOpMAtsBzPBv4A9DOdpYgtBX4O3Oe7tvxCtoIUeBssx+sJ3AqcbDpLCXkZOMt37fdNB4kLKfBWWI73A+BmoIfpLCWoCbiBYFqdMh0m6qTAzViO9w3gjwTnKQuzZgCn+K79iekgUSYX9IcsxzsBmIOUNypGAG9bjneK6SBRVvIjsOV4FYBLcJRZRNODwE9l0YEtlXSBLcfbCfg7cKjpLKJFHwIn+679lukgUVKyU2jL8Q4BZiLljYvdgemW433PdJAoKckCW453PvA8sLPpLKJVaoC/W453uekgUVFyU2jL8W4kOKNKxNufCN4Xl/RHTSVT4PBa3T8Bp5vOInJmMvB937VXmQ5iSkkU2HK8auBvwHGGo4jcewc42nfthaaDmFD0BQ7XVJ4EjDUcReTPfGCM79qLTQcptKIucLhSxhRgf9NZRN7NBcb6rr3UdJBCKtoCW47XmeBIs5S3dMwBDiultauL8mMky/FqgSeR8paaIcCzluN1NR2kUIquwOGpkY8iJ2iUqqHAlHAGVvSKrsDA/wK26RDCqOHAE5bjFf2yR0VVYMvx6pHPeUVgLMFKKkWtaA5ihZedPWw6h4icS3zX/p3pEPlSFAUOb2MyA6gznUVETgY4xnftKaaD5EPsC2w5XkfgDWCQ6SwislYCBxTj6h7F8B74bqS8Yvt2AP7PcrwOpoPkWqwLbDneecAE0zlELOwDFN174dhOoS3HGwG8BFSZziJiZVwxvR+OZYHDOyXMoTTuuytyawGwt+/aX5kOkgtxnUJfg5RXtE0fgtu5FIXYjcCW4x0A/Bu5K6BonxN9137MdIj2ilWBw/Oc3wT2NZ1FxN5ygqn0EtNB2iNuU+hLkPKK3OhBERyVjs0IbDneAIIDVzWms4iioYGRvmu/bjpIW8VpBL4TKa/ILUVwE7vYikWBLcc7HDjKdA5RlA62HO/7pkO0VSwKDFxnOoAoajeEK5fGTosFVkpZSql3CxFmayzHG09wpzoh8sUCLjQdoi0iPQJbjlcGXGs6hygJvwhXMY2VbAtcrpS6Syn1nlJqqlKqVin1Y6XUG0qp2Uqpx5RSHQCUUvcrpf6glJqmlPpEKTVGKXWvUmquUur+VuY7Bdi7ldsI0RadgHNNh2itbAu8O3Cn1now8BVwAvBPrfWBWut9CdbkPaPZ83cAvkkwLXmS4EjfYGCIUmq/bHYYrmdUn2U+IXLh3HBF09jItsCfaq1nhV+/RfCeYW+l1MtKqTnAqQQFXe9JHXzAPAdYorWeo7XOAO+F22bjJKB/ls8VIhd6AKeZDtEa2Ra4sdnXaaACuB84V2s9BLiaTT+jXf/8zGbbZsJts3FBls8TIpcuCo+9xEJ7gnYCFimlKglG4JyxHG80MCyXrylElgYA3zUdIlvtKfAvCRaSexb4IDdxNvhZjl9PiNaIzf2jI3cutOV4vYDPgaJflFtE2sG+a79mOkRLojjXPx0przDvNNMBshGpEdhyPAV8jKy2IcxLADv5rt1gOsj2RG0EHoWUV0RDF+A40yFaErUCH286gBDN5PTTlXyIWoFjc/helISjon6v4cgU2HK8/cn+LC0hCqGKiA8qkSkwMn0W0XSi6QDbIwUWYvvGRPlG4dmel5xXluPtRR5vUPb1G4+zevZUUFDZ06LHMRew3LuZphVfAJBpWENZTR29J96+yXY6lWTxXy5Hp5ogk6HDHqPoekhwXGPZEze0uL0oCnUEC0pMNx1kayJRYOBb+Xrh1KrlfP3Wk/Q+4/eUVVaz7HGXNXNfouf4yzc8Z8Xzd1NWvZVbC5dX0uukX1NWVYtOp1j88GXU7rY/1X0GZbe9KBZHENECR2UKPSqvr55Jo1NJdCaNTjVS3rHbhh9prVn7wXTq9jx0i82UUpRVBZeH6kwKMmlQapPnbG97UTQONx1gW6IyAh+crxeu6NSDzsO/y4I/TERVVFGz61Bqd914oVPjF+9RXteVym59trq9zqRZ9MAFpFYuotMwm+ree2zy85a2F0VhhOV4HX3XXm06yOaMj8CW4+0K7Jyv1083rGbthzPoc9Y99P3pg+imRla/N23Dz9e8/+J2R09VVk7vibfT95z7aVw0n+Qyf5Oft7S9KAqVQCT/JxsvMHmePjf4s6jo0ovyDl1Q5RV0GHgQjQvmAsHounb+a3QY1PL/m7KajtT0G8K6T2Zu+F5rthexd5jpAFsThQLnbfoMUNG5J8mF88g0NaC1puGz2VR27wcE5a7s3peKzj22um16bYJMQzBryjQ10vBZ8Pz1WtpeFJWhpgNsTRTeA+d1BK7uvQcd9hjFovsvQJWVUdWrP532HQfAmrkvbTH9Ta36ki8n30av711NevUKlns3g86AztBh0CF0GDB8w3O3tr0oWpFcHdXo5YSW49UAa4jGTECIlvT0XXu56RDNmS5O/whkECJbkRuFTZdngOH9C9EaQ0wH2JwUWIjsyQi8GSmwiBMp8GakwCJOdjEdYHNSYCGy19N0gM0Z+xgpvMZyHVBuJIAQbdPNd+2VpkOsZ3IE7o6UV8TPjqYDNGeywDsY3LcQbSUFDkmBRRxJgUNdDe5biLaSAodkDRoRR91afkrhmCxwB4P7FqKtIrVCpRRYiNaJ1CcnJgts+iQSIdoiCtfQb2AyTKPBfRe1XqxYOq364pU1JCN35lDcNVKZhGWmY2xgssCRvu9qnC2h247HJq9dN7XqsmS50juZzlNMaklGauZoMowUOI8+1n12OSZ5/bq0VktNZykyKdMBmpMCF7F5+hu7fjv561UZraIz54u/JtMBmpMCF7m5epf+xyavTWS0itRaTjEWqd9bkwVeZ3DfJeU9veuA7yavXpHRaoXpLEVgsekAzZks8FqD+y45s/WAgScmr1qa0XxlOkvMLTIdoDmTBV5ocN8laaYeOGhC8spFWpMwnSXGIvV7a7LAS4nY+4lS8IYetOfJTVd8oTVfm84SUzICA/iurYHPTe2/lP07M3jwD5t+/pnWrDKdJWbWUJ+I1D98pj+U/o/h/Zes6ZkhQyY2Xfap1qwxnSVGIjX6ghS4pL2Q2W+fM5su/lBrOaCYpUi9/wUpcMl7LrP/fmc1XTBPa/lYLwsyAm/mM8P7F8CUzPCh5zb97H2t5aBiC+aZDrA50wV+3/D+RcjLjNz/wqZz3tVarhLbjrdMB9ic6QLPImLnlpayxzOjD7g09ZPZWpM0nSWi3jQdYHNG7w8MYDneTCJ69/NSdVL58zOur7h7mFLtXz7m9CfW8dT8FDvWKd49pyMAl05t4Mn5KarKoX+3Mu4bX0vXGrXFtl81aM6ctI53l2ZQCu79Tg0H9avgl8838MS8FGUKdqxT3H9cLb075X0sWkR9one+d9JapkdggDdMBxCb+lv6myOuTJ32ltbtv3TutP0qmfyDTVdPOrJ/Be+eU8c7Z3dkYLcyrn9567P28yc3MG5ABR+c25HZZ9WxZ89gNZtLR1XzztkdmXVWR749sIJrXizIrD9y02eQAotteCh91MhrUj98Q2vS7XmdQ3epoFvtpqPrUf0rqCgLvjeybzlfrMpssd3XjZqXPktxxtBgElBVrjaM0p2rN77emiRsOXbnRSQLHIX1faTAEXVf+uiDKsi8+ouKh0colZ/F3O6d1cSEwVv+Gn6yMkPPDoqJTzQwe0ma/Xcu59ZxNdRVBXW94rkGHnyniS7Vimk/Ksj6iJEscBRG4PeQSwsj6660ffBvUhP+rTVbDpPtdN1LjVSUwalDtnyrncrAzEUZzj6gkrd/0pG6SoU7feNU+brDa/j8wk6cOqSSO14vyDG3yB3AgggU2HftFPC66Rxi236fHj/qltQJr2pNzo54PjAryVMfpnj4+FqU2nIS3Lezom9nxYi+weh84l4VzFy85b8hpwyp5LG5eV/l5nPqE5E7iQMiUODQVNMBxPbdmj5h9J3p8dNzUeLJH6W44ZUkk06qpUPl1t/B7tSxjH5dypi3PHgL/tynKfbqEfy6fvjlxrflk+alGNQj77/GT+V7B21l/GMkAMvxhhHR9xhiU07FX146q+KpQ7N9/smPreUFP83ytZpedYqrx1Zz/fRGGtPQvXbjgaw/fruWhasynDmpgadPDd7Tzlqc5sxJ60imYbcdgo+bdqhVnPDIWuYtz1CmYJeuZfzRrqFP57yWeBz1iSn53EFbRaXAiuA8016ms4iW/bLioRfPqHhmjOkcBbIK6EF9IpInt0RiCh1eG+yZziGy86vUD8c8mDryRdM5CmRyVMsLESlw6HHTAUT2rkxNHPPX1GGlUOInTAfYnigV+FlgtekQIns/T/14zD/Sh75gOkcepYCnTYfYnsgU2HftBmQaHTuXNJ01dlL6oBdM58iTl6lPrDQdYnsiU+DQvaYDiNb7WdN5Y59JH1iM0+nHTQdoSdQK/Czgmw4hWu/spgvHPJse9oLpHDmUBP5iOkRLIlXg8Gj0PaZziLb5cdMlY6el933BdI4c+Sf1icjfjiZSBQ7dB+27AkaYM7Hp8rGvpAcXw3T6T6YDZCNyBfZdewHwjOkcou1ObbpizIzMoDiXeD71iWmmQ2QjcgUO3WU6gGifCckrx7yZGfiS6RxtdLvpANmKaoE9ZMXK2DsxedUhszL9Xzado5W+Ingbt11KqTqllKeUmq2UelcpNUEp5SulblBKvR4+BoTPPVYpNUMp9bZS6l9KqV7h9+uVUg8opaaG2x6vlLpRKTVHKTVZKdXikkaRLLDv2mngetM5RHspdVzymtHvZqw4lfge6hPZ3K1iHLBQa72v1npvYHL4/a+11sOBO4Bbwu9NB0ZqrYcCfwMua/Y6/QEbGA/8GZimtR5CcI283VKISBY4dB9y76QioNSxyWtHfZDpN910kiw0kf30eQ5wRDjiHqK1Xn/Hx782+/Og8Ou+wBSl1BzgUmBws9d5RmvdFL5eORv/IZgDWC2FiGyBfddOIqNwUdCUlR2dvP7g+Zk+r5jO0oK7qE9k9dZNaz0f2J+gaNcrpa5c/6PmTwv/vB24IxxZfwLUNHtOY/h6GaBJb7w8MEMWS15FtsChe4AvTIcQ7acpKxuXvGHkJ5mdXzWdZRtWA1dn+2SlVG9grdb6z8BvgWHhjyY0+/O18OsuwILw6x+1P+pGkS5wOAq7pnOI3MhQVn5k8sbhfqbXay0/u+B+R31iaSuePwR4XSk1C7gCuDb8frVSagZwPnBh+L164FGl1MtATk8OicQF/dtjOV418DHQx3QWkRsVpJqmVV00s1/Z8hGms4SWAAOoT7TrajillA8coLUu2BlckR6BAXzXbgT+x3QOkTspKioPS940bIHuHpXFDH/V3vKaEvkChx4gOBQvikSKisqxjTfvt1jvYHpd8I/I0WmTWmurkKMvxKTA4UUO50D7b/UhoqOJiqpDG2/ZZ6nuYnJBwyuoT8T2BnuxKDCA79pzgNtM5xC5laSy+tDGW/ZarjvPNLD7F4FHDew3Z2JT4NBVbDwcL4pEA9W1hzTeOmiF7jSrgLtdDUykPhHto7gtiFWBfddeDVxkOofIvXVUdxjdeOvuX+m62QXa5SXUJz4t0L7yJvIfI22N5XgecIzpHCL36li3+tXq8z7totYOyeNuplKf+FYeX79gYjUCN3MGsMx0CJF7a6jtOKrxtl1W6dr38rSLBMHvT1GIZYF9114MnG46h8iP1XTofHDjbX1X65r38/Dy51OfKJrTc2NZYADftZ8C7jSdQ+THKuq6jGq8rfdaXf1BDl92EvWJB3L4esbFtsChiwETHz+IAkjQsevBjbf1Wqur5uXg5RYC/52D14mUWBc4PM3y+8DXprOI/PiKTjuMbry1R4Ou/LAdL7MOGE99YkmuckVFrAsM4Lv2xwSXaOX8DvIiGlbQpfvoxlu7NujKj9uwuQZOoz7xZq5zRUHsCwzgu/bjBCsdiCK1nK49xzTe3KlRV7T2s9trqE88kpdQEVAUBQbwXfsmYrSaoGi9JXTbcWzjzbVJXeFnuckjtOIi/TgqmgKHLiDit4MU7bOI7juNbbypukmXt7T0zZsEU+f4nanUCrE8E2t7LMerBaYBUblYXORBX7Vs4bSqi9KVKt1vKz9eCBxIfWJhoXMVWrGNwPiuvQ44FvjEdBaRP1/onr0PT/5WpXTZ5he3LAfGlUJ5oQgLDOC79jLgCOROh0XtP7pX3yOTN6ZTumxR+K0VwBHUJ+aYzFVIRTeFbs5yvH7Ac8DuprOI/OmvFnw2ucppqFTpk6lPvG06TyEVdYEBLMfbiaDEe5nOIvLmy1oaj5zrHl9S5YUSKDCA5Xg9ganAfoajiNxbDBzhu3a+rl6KtKJ8D7y58D3xN4GorIIocsMHDinV8kKJFBjAd+2VwJHA06aziJx4GRjuu/ZHpoOYVDIFBvBd+2uCj5jkbg/xdg/BtLnkF3UoiffAW2M53kkEvwgdTGcRWUsDl/iufYvpIFFRsgUGsBxvKPA48A3DUUTLEsAE37WnmA4SJSU1hd6c79pvAwcQrA8sousdYISUd0slXWDYcIT6cII1p+XOD9GSAW4ADvRdOxerchSdkp5Cb85yvOHAQ8BA01kEnwL/5bu23BNrO0p+BG7Od+3XgaHALcgKHybdA+wj5W2ZjMDbYDneQcC9wCDTWUrIQuBs37UnmQ4SFzICb4Pv2q8RnHr5c2TRvHxbR3CH+4FS3taRETgLluPtCNQTLEtabjZN0fk7cJnv2v8xHSSOpMCtYDnensBvANt0liLwJnCB79qvmA4SZ1LgNrAc73DgRmCY6SwxNI9guvxweON20Q5S4HawHO8I4BKgKO50l2ezgeuAx3zXliP8OSIFzgHL8fYmuM3LKUCV4ThRMwW4yXftqaaDFCMpcA5ZjrczcB7Bwa7uhuOYtJLg4NQdpXytbiFIgfPAcrxKgmn1KcB3gDqziQoiCXgEZ7J5vmsnDecpCVLgPLMcrw4YT1Dmo4BKs4ly7lWC0j7iu/YK02FKjRS4gCzH605Q5m8RXEARx2n2MoJFAp8FnvVd+3PDeUqaFNgQy/HKCD6GOgw4FBgNdDWZaRvWESxf8y+C0s6Wj3+iQwocEWGh9wKGAIObPfpTuFNevyC49nb9Yw7wge/acpllREmBI85yvBqCCyoGA/2AHkDPZo/1f9/agTIdPjIEK1osDx/LCC4cWBA+PgXmhAv/iRiRAhcJy/HWn6OdkSlu6ZACCxFjcjmhEDEmBRYixqTAQsSYFFiIGJMCCxFjUmAhYkwKLESMSYGFiDEpsBAxJgUWIsakwELEmBRYiBiTAgsRY1JgIWJMCixEjEmBhYgxKbAQMSYFFiLGpMBCxJgUWIgYkwILEWNSYCFiTAosRIxJgYWIMSmwEDEmBRYixqTAQsTY/wOBKU2tofgq3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4094868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can see 88% data is not spam and 13% is spam \n",
    "#data is imbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "045d65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create 3 columns in which we can find number of characters , no. of words and no. of sentences in the sms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4893da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07942820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb810cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters']=df['text'].apply(len)  #no of characters in sms columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de4367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no. of words \n",
    "df['num_words']=df['text'].apply(lambda x : len(nltk.word_tokenize(x)))  #breaking sentences into words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b077da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  num_characters  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...             111   \n",
       "1       0                      Ok lar... Joking wif u oni...              29   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...             155   \n",
       "3       0  U dun say so early hor... U c already then say...              49   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...              61   \n",
       "\n",
       "   num_words  \n",
       "0         24  \n",
       "1          8  \n",
       "2         37  \n",
       "3         13  \n",
       "4         15  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b341c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'].apply(lambda x:nltk.sent_tokenize(x))  #breaks sentences in the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "374cb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of sentences \n",
    "df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6c11a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  num_characters  \\\n",
       "0       0  Go until jurong point, crazy.. Available only ...             111   \n",
       "1       0                      Ok lar... Joking wif u oni...              29   \n",
       "2       1  Free entry in 2 a wkly comp to win FA Cup fina...             155   \n",
       "3       0  U dun say so early hor... U c already then say...              49   \n",
       "4       0  Nah I don't think he goes to usf, he lives aro...              61   \n",
       "\n",
       "   num_words  num_sentences  \n",
       "0         24              2  \n",
       "1          8              2  \n",
       "2         37              2  \n",
       "3         13              1  \n",
       "4         15              1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5a04e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.169000e+03</td>\n",
       "      <td>5169.000000</td>\n",
       "      <td>5169.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.598052e+04</td>\n",
       "      <td>7451.501064</td>\n",
       "      <td>1.948152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.645006e+05</td>\n",
       "      <td>29833.936454</td>\n",
       "      <td>1.363792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.300000e+01</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.320000e+02</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.905320e+06</td>\n",
       "      <td>553318.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_characters      num_words  num_sentences\n",
       "count    5.169000e+03    5169.000000    5169.000000\n",
       "mean     6.598052e+04    7451.501064       1.948152\n",
       "std      2.645006e+05   29833.936454       1.363792\n",
       "min      2.000000e+00       1.000000       1.000000\n",
       "25%      3.700000e+01       9.000000       1.000000\n",
       "50%      6.300000e+01      15.000000       1.000000\n",
       "75%      1.320000e+02      29.000000       2.000000\n",
       "max      4.905320e+06  553318.000000      28.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "333099bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max char - 910 , word-220 and sentences -28 used in a text column \n",
    "#on an average 79 char , 18.5 words and 1.98 sentences used in a text xolumn \n",
    "# min char - 2 , word-1 and sentences -1 used in a text column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252e7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.516000e+03</td>\n",
       "      <td>4516.000000</td>\n",
       "      <td>4516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.002508e+04</td>\n",
       "      <td>4523.619575</td>\n",
       "      <td>1.799601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.300520e+05</td>\n",
       "      <td>25948.391783</td>\n",
       "      <td>1.278465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.905320e+06</td>\n",
       "      <td>553318.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_characters      num_words  num_sentences\n",
       "count    4.516000e+03    4516.000000    4516.000000\n",
       "mean     4.002508e+04    4523.619575       1.799601\n",
       "std      2.300520e+05   25948.391783       1.278465\n",
       "min      2.000000e+00       1.000000       1.000000\n",
       "25%      3.400000e+01       9.000000       1.000000\n",
       "50%      5.400000e+01      14.000000       1.000000\n",
       "75%      1.010000e+02      24.000000       2.000000\n",
       "max      4.905320e+06  553318.000000      28.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for ham \n",
    "df[df['target']==0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9ecb647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_characters</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.530000e+02</td>\n",
       "      <td>653.000000</td>\n",
       "      <td>653.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.454825e+05</td>\n",
       "      <td>27700.065850</td>\n",
       "      <td>2.975498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.887254e+05</td>\n",
       "      <td>43844.536138</td>\n",
       "      <td>1.487993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.370000e+02</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.560000e+02</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.759950e+05</td>\n",
       "      <td>64978.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.852054e+06</td>\n",
       "      <td>321700.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_characters      num_words  num_sentences\n",
       "count    6.530000e+02     653.000000     653.000000\n",
       "mean     2.454825e+05   27700.065850       2.975498\n",
       "std      3.887254e+05   43844.536138       1.487993\n",
       "min      1.300000e+01       2.000000       1.000000\n",
       "25%      1.370000e+02      27.000000       2.000000\n",
       "50%      1.560000e+02      32.000000       3.000000\n",
       "75%      5.759950e+05   64978.000000       4.000000\n",
       "max      2.852054e+06  321700.000000       8.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for spam\n",
    "df[df['target']==1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84fb6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can see that length of spam message is larger than ham messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9571c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets visualize through graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b11fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(\u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_characters\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_characters\u001b[39m\u001b[38;5;124m'\u001b[39m],color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target']==0]['num_characters'])\n",
    "sns.histplot(df[df['target']==1]['num_characters'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target']==0]['num_words'])\n",
    "sns.histplot(df[df['target']==1]['num_words'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see relationship between two columns \n",
    "sns.pairplot(df,hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with target highest correlation is 0.38 which is num_characters \n",
    "#since correlation between three columns(target is not included ) is very high which shows strong correlation so we cant keep all the\n",
    "#columns and we will only keep num_characters as it has high correlation with target as compares to num_words or num_sentences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4ad34",
   "metadata": {},
   "source": [
    "# 3.Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case \n",
    "# tokenization \n",
    "# Removing special characters'\n",
    "# Removing stop words and punctuation\n",
    "# stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string  \n",
    "ps=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y=[]\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6409e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transformed_text']=df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create a word cloud \n",
    "# in this all the important words will be highlighted(make the words big ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "spam_wc = wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69637cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wc = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10929336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to find top 30 words used in spam or ham respectively  \n",
    "#the first step is to split the word in list in transformed text column which is spam and put it in alist \n",
    "spam_corpus=[]\n",
    "for msg in df[df['target']==1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba763cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will find occurence of each word in spam_corpus \n",
    "from collections import Counter \n",
    "Counter(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to find most common 30 words in the spam corpus\n",
    "Counter(spam_corpus).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e51086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the above to a DataFrame\n",
    "pd.DataFrame(Counter(spam_corpus).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will plot barplot between 0 and 1st columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing same for ham \n",
    "ham_corpus=[]\n",
    "for msg in df[df['target']==0]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        ham_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d84349",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ham_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37926e",
   "metadata": {},
   "source": [
    "# Model Building \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10394da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use naive bayes here as it is believed that naive bayes performs better on textual data \n",
    "#we will use further ensemble learning to imporove the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(df['transformed_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape  #5169 sms and 6717 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeec76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score  #this is a high precision model in which we want to reduce true positive\n",
    "gnb=GaussianNB()\n",
    "bnb=BernoulliNB()\n",
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de798b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5721f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score of gaussian is very low\n",
    "#precision is how good is the model in predicting specific category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeda55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy is good but precision score is still low and in this precision score is important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy is good and precision is also good so bernoulli is best here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now using tfidf vectorizer instead of count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2de427",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1709aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#y will be same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18861135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017afcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score of gaussian is very low\n",
    "#precision is how good is the model in predicting specific category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here precision performs really better as its not giving any false positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a980de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can go either with mnb or bnb but we will go with mnb as its precision is very good although accuracy is low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we choose tfidf --> MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we bring many ML models and compare it with Multinomial MNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad163b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs={\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    return accuracy ,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier(svc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da72c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores=[]\n",
    "precision_scores=[]\n",
    "\n",
    "for name,clf in  clfs.items():\n",
    "    print('name',name)\n",
    "    current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
    "    print(current_accuracy)\n",
    "    print(current_precision)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we consider Naive bayes as it has highest precision \n",
    "#we can too use random forest but in terms of textual data we find Naive bayes better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41342c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad471d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Algorithm',y='value',hue='variable',data=performance_df1,kind='bar',height=5)\n",
    "plt.ylim(0.5,1.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae524a",
   "metadata": {},
   "source": [
    "# Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. change max_features of tfidf vectorizer \n",
    "#tfidf=TfidfVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4df8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2=tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#y will be same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f55554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde481c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnb.fit(X_train,y_train)\n",
    "#y_pred1=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "#print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "#print('precision score of gaussian',precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnb.fit(X_train,y_train)\n",
    "#y_pred2=mnb.predict(X_test)\n",
    "#print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "#print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "#print('precision score of multinomial',precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf80653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bnb.fit(X_train,y_train)\n",
    "#y_pred3=bnb.predict(X_test)\n",
    "#print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "#print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "#print('precision score of bernoulli',precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clfs={\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    return accuracy ,precision'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a72cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''accuracy_scores=[]\n",
    "precision_scores=[]\n",
    "\n",
    "for name,clf in  clfs.items():\n",
    "    print('name',name)\n",
    "    current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
    "    print(current_accuracy)\n",
    "    print(current_precision)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n",
    "    \n",
    "print(accuracy_scores)\n",
    "print(precision_scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd983b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "''''temp_df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df=performance_df.merge(temp_df,on='Algorithm')  #merging both dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142deff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can see accuracy of naive bayes is increased in accuracy_max_ft_3000\n",
    "#by seeing this we can say that the most powerful algo in this dataframe is NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65963a6",
   "metadata": {},
   "source": [
    "# 2nd Improvement -applying scaling in temp_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf=TfidfVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X3=tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#y will be same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb44fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X3 = scaler.fit_transform(X3)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1efafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X3,y,test_size=0.2,random_state=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d63a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80283a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21344c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61196734",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clfs={\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    return accuracy ,precision'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d40d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''accuracy_scores=[]\n",
    "precision_scores=[]\n",
    "\n",
    "for name,clf in  clfs.items():\n",
    "    print('name',name)\n",
    "    current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
    "    print(current_accuracy)\n",
    "    print(current_precision)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n",
    "    \n",
    "print(accuracy_scores)\n",
    "print(precision_scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5807fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''new_df_scaled = new_df.merge(temp_df,on='Algorithm')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0738038",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''new_df_scaled'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e94032e8",
   "metadata": {},
   "source": [
    "#not having any impact also on the other hand decreased the precision value \n",
    "#we will not keep it further "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f05f6",
   "metadata": {},
   "source": [
    "# 3rd improvement adding df['num_char'] with the input column in tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tfidf=TfidfVectorizer(max_features=3000)\n",
    "X4=tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#appending the num_character col to X\n",
    "X4 = np.hstack((X4,df['num_characters'].values.reshape(-1,1)))\n",
    "X4.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4dbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X4,y,test_size=0.2,random_state=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe211aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a858801",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfccae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''clfs={\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    return accuracy ,precision'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d16a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''accuracy_scores=[]\n",
    "precision_scores=[]\n",
    "\n",
    "for name,clf in  clfs.items():\n",
    "    print('name',name)\n",
    "    current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
    "    print(current_accuracy)\n",
    "    print(current_precision)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n",
    "    \n",
    "print(accuracy_scores)\n",
    "print(precision_scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae221631",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''new_df_scaled.merge(temp_df,on='Algorithm')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#can see only accuracy_max_3000_ft and precision_score_3000 ft performs best rest improvement does not have any impact  \n",
    "so made this improvement as markdown '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier , Stacking Classifier\n",
    "svc =SVC(kernel = 'sigmoid',gamma = 1.0,probability = True)\n",
    "mnb=MultinomialNB()\n",
    "etc=ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
    "voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3679772",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce39c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stacking\n",
    "estimators=[('svm', svc), ('nb', mnb), ('et', etc)]\n",
    "final_estimator=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5653316",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fe19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot see any impact of stacking or voting ensemble techniques so we decided to go for 1st improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5959299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
