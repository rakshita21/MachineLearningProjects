{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "df = pd. read_csv('spam.csv', encoding='ANSI')\n",
    "df. to_csv('spam.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44def8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a95a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Data Cleaning \n",
    "#2. EDA \n",
    "#3. Text Preprocessing \n",
    "#4. Model Building \n",
    "#5. evaluation\n",
    "#6. improvements \n",
    "#7. creating website \n",
    "#8 . deploy to heroku "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83b6dd",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59eac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming the columns \n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since in target columns its written either ham or spam so we apply label encoding here \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()\n",
    "df['target']=encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec740359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam = 1 , ham =0 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values \n",
    "df.isnull().sum()  #no missing values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973061e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicate values \n",
    "df.duplicated().sum() #403 duplicate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates \n",
    "df=df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa90dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()  #no duplicates present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986542b",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()  #ham is 4516 and spam is 653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4094868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can see 88% data is not spam and 13% is spam \n",
    "#data is imbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create 3 columns in which we can find number of characters , no. of words and no. of sentences in the sms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4893da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07942820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb810cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters']=df['text'].apply(len)  #no of characters in sms columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no. of words \n",
    "df['num_words']=df['text'].apply(lambda x : len(nltk.word_tokenize(x)))  #breaking sentences into words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b077da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b341c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['text'].apply(lambda x:nltk.sent_tokenize(x))  #breaks sentences in the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of sentences \n",
    "df['num_sentences']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c11a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333099bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max char - 910 , word-220 and sentences -28 used in a text column \n",
    "#on an average 79 char , 18.5 words and 1.98 sentences used in a text xolumn \n",
    "# min char - 2 , word-1 and sentences -1 used in a text column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ham \n",
    "df[df['target']==0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for spam\n",
    "df[df['target']==1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can see that length of spam message is larger than ham messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets visualize through graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b11fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target']==0]['num_characters'])\n",
    "sns.histplot(df[df['target']==1]['num_characters'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target']==0]['num_words'])\n",
    "sns.histplot(df[df['target']==1]['num_words'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see relationship between two columns \n",
    "sns.pairplot(df,hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with target highest correlation is 0.38 which is num_characters \n",
    "#since correlation between three columns(target is not included ) is very high which shows strong correlation so we cant keep all the\n",
    "#columns and we will only keep num_characters as it has high correlation with target as compares to num_words or num_sentences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4ad34",
   "metadata": {},
   "source": [
    "# 3.Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case \n",
    "# tokenization \n",
    "# Removing special characters'\n",
    "# Removing stop words and punctuation\n",
    "# stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string  \n",
    "ps=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y=[]\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6409e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f4b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transformed_text']=df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef3afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will create a word cloud \n",
    "# in this all the important words will be highlighted(make the words big ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc=WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "spam_wc = wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69637cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wc = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10929336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to find top 30 words used in spam or ham respectively  \n",
    "#the first step is to split the word in list in transformed text column which is spam and put it in alist \n",
    "spam_corpus=[]\n",
    "for msg in df[df['target']==1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba763cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will find occurence of each word in spam_corpus \n",
    "from collections import Counter \n",
    "Counter(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to find most common 30 words in the spam corpus\n",
    "Counter(spam_corpus).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e51086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the above to a DataFrame\n",
    "pd.DataFrame(Counter(spam_corpus).most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will plot barplot between 0 and 1st columns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing same for ham \n",
    "ham_corpus=[]\n",
    "for msg in df[df['target']==0]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        ham_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d84349",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ham_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37926e",
   "metadata": {},
   "source": [
    "# Model Building \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10394da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use naive bayes here as it is believed that naive bayes performs better on textual data \n",
    "#we will use further ensemble learning to imporove the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(df['transformed_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape  #5169 sms and 6717 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeec76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score  #this is a high precision model in which we want to reduce true positive\n",
    "gnb=GaussianNB()\n",
    "bnb=BernoulliNB()\n",
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de798b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5721f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score of gaussian is very low\n",
    "#precision is how good is the model in predicting specific category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeda55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy is good but precision score is still low and in this precision score is important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy is good and precision is also good so bernoulli is best here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now using tfidf vectorizer instead of count vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2de427",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1709aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#y will be same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18861135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train,y_train)\n",
    "y_pred1=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of gaussian', accuracy_score(y_test,y_pred1))\n",
    "print('Confusion matrix of gaussian',confusion_matrix(y_test,y_pred1))\n",
    "print('precision score of gaussian',precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017afcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision score of gaussian is very low\n",
    "#precision is how good is the model in predicting specific category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2=mnb.predict(X_test)\n",
    "print('accuracy of multinomial', accuracy_score(y_test,y_pred2))\n",
    "print('Confusion matrix of multinomial',confusion_matrix(y_test,y_pred2))\n",
    "print('precision score of multinomial',precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here precision performs really better as its not giving any false positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a980de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3=bnb.predict(X_test)\n",
    "print('accuracy of bernoulli', accuracy_score(y_test,y_pred3))\n",
    "print('Confusion matrix of bernoulli',confusion_matrix(y_test,y_pred3))\n",
    "print('precision score of bernoulli',precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can go either with mnb or bnb but we will go with mnb as its precision is very good although accuracy is low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we choose tfidf --> MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we bring many ML models and compare it with Multinomial MNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad163b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs={\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    return accuracy ,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier(svc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da72c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores=[]\n",
    "precision_scores=[]\n",
    "\n",
    "for name,clf in  clfs.items():\n",
    "    print('name',name)\n",
    "    current_accuracy,current_precision=train_classifier(clf,X_train,y_train,X_test,y_test)\n",
    "    print(current_accuracy)\n",
    "    print(current_precision)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df=pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we consider Naive bayes as it has highest precision \n",
    "#we can too use random forest but in terms of textual data we find Naive bayes better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41342c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad471d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Algorithm',y='value',hue='variable',data=performance_df1,kind='bar',height=5)\n",
    "plt.ylim(0.5,1.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae524a",
   "metadata": {},
   "source": [
    "# Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccb458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3679772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc575e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce39c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883afb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5653316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fe19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5959299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
